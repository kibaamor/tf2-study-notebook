{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.2.4)\n",
      "pip 20.2.4 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/\n",
    "!python -m pip install -U pip\n",
    "!pip -V\n",
    "!pip install sklearn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.2\n",
      "numpy 1.18.5\n",
      "pandas 1.1.3\n",
      "sklearn 0.23.2\n",
      "tensorflow 2.3.1\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=(None, 5))\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 0.02719174, -0.01928368,  0.04987185,  0.12432446,  0.163648  ,\n",
       "          0.17825444,  0.0766273 , -0.14948672, -0.09815107, -0.18293384,\n",
       "          0.189492  , -0.19046198, -0.02438176,  0.00065862, -0.03288607,\n",
       "         -0.20381474,  0.0809025 ,  0.05971031, -0.06390099,  0.20126392,\n",
       "          0.10442515, -0.08025752, -0.12754291, -0.03322466, -0.14739892,\n",
       "         -0.06203733, -0.10888165,  0.17118715, -0.2190864 ,  0.14295648,\n",
       "          0.128172  ,  0.03769387, -0.23745328,  0.10251434, -0.12241895,\n",
       "          0.20820023, -0.07966068, -0.10952659,  0.2107587 , -0.1660225 ,\n",
       "         -0.16035801, -0.21440768,  0.21816264, -0.07816896, -0.04375541,\n",
       "         -0.17587474, -0.0789514 ,  0.23085158,  0.20169647,  0.18418579,\n",
       "          0.16097234, -0.16012937,  0.1181931 ,  0.02235536, -0.0581129 ,\n",
       "          0.19694372, -0.03965946, -0.17071494, -0.21489257,  0.1371452 ,\n",
       "          0.05803488,  0.22430952,  0.07012115,  0.11923198, -0.21662037,\n",
       "         -0.02497362,  0.06354512,  0.18268539,  0.01562856, -0.18432179,\n",
       "          0.22569628,  0.12497254,  0.00805156, -0.04664673, -0.16171366,\n",
       "          0.19382383,  0.1432174 ,  0.00522859,  0.2308502 , -0.18766922,\n",
       "          0.13271497, -0.07419124,  0.06548615, -0.10946845, -0.18443498,\n",
       "          0.14575683,  0.10349183, -0.0566185 , -0.0966019 ,  0.11870496,\n",
       "          0.04689322,  0.03063832, -0.13590258, -0.08662653, -0.15645725,\n",
       "          0.18362288,  0.16458826, -0.07040879,  0.00623833,  0.03060161],\n",
       "        [-0.02730064, -0.18995422, -0.06587495,  0.17873774,  0.10068817,\n",
       "         -0.09015189,  0.02950634, -0.22970253,  0.08101381,  0.07214291,\n",
       "         -0.0540525 , -0.01023485,  0.10827132,  0.18372075, -0.19447428,\n",
       "         -0.13264892, -0.21449636,  0.20544337, -0.04758471, -0.00334109,\n",
       "          0.23381416,  0.07007937,  0.23412164,  0.01763926, -0.14015293,\n",
       "         -0.19590521,  0.23892184, -0.00934894,  0.13549553,  0.19172375,\n",
       "         -0.09078281,  0.17614745,  0.05625649, -0.13577703, -0.23708734,\n",
       "          0.05735736, -0.16272858, -0.05732709, -0.03160977,  0.05725585,\n",
       "         -0.18924586,  0.23182939, -0.15696865, -0.05098976,  0.10063152,\n",
       "          0.16395326, -0.11244553, -0.01576225,  0.02095698,  0.02170725,\n",
       "          0.01291437, -0.1343738 ,  0.1319377 , -0.11355764, -0.11555176,\n",
       "         -0.12186829, -0.02349472,  0.00129311,  0.18251728,  0.09911944,\n",
       "          0.02185605, -0.17393082, -0.0125768 , -0.16738763,  0.12105449,\n",
       "          0.11951648,  0.1865396 , -0.22825177, -0.0846018 , -0.06851025,\n",
       "          0.2193542 ,  0.16437112, -0.02575079, -0.0958693 , -0.14509177,\n",
       "          0.05346696, -0.2299015 ,  0.22406615, -0.1915958 , -0.13875735,\n",
       "          0.20972042,  0.09743108,  0.02530356,  0.00924443,  0.16090207,\n",
       "          0.05022784,  0.13641892, -0.05822758, -0.20450567, -0.08271386,\n",
       "          0.06250872,  0.102906  , -0.17315754, -0.18041354,  0.15698488,\n",
       "         -0.21120137, -0.05532783,  0.21393634,  0.10622351, -0.00342704],\n",
       "        [-0.01583862,  0.00425321,  0.0542623 , -0.1706282 ,  0.04547895,\n",
       "          0.21048222, -0.0345979 ,  0.20474721, -0.02695629,  0.12915786,\n",
       "          0.04251246,  0.20724417, -0.20344138,  0.17376406,  0.00492191,\n",
       "         -0.12265656,  0.208557  , -0.13009071, -0.09545076, -0.1061697 ,\n",
       "          0.0587735 ,  0.02542357,  0.22611575, -0.03000405, -0.03043458,\n",
       "          0.17610072, -0.05384392,  0.1124898 , -0.08470275, -0.15204468,\n",
       "          0.17209236, -0.18710755, -0.04353708, -0.09145087, -0.23395643,\n",
       "          0.03284653, -0.03582963, -0.00560959,  0.05590801,  0.01110308,\n",
       "         -0.20655143,  0.0689915 , -0.00234686,  0.01344623,  0.00338641,\n",
       "         -0.23025525, -0.09761272, -0.01346755,  0.01351754, -0.12646478,\n",
       "         -0.18519229, -0.06681933, -0.00755675, -0.07728721,  0.14800502,\n",
       "         -0.13734922,  0.10834666,  0.14656772,  0.10628597, -0.05586392,\n",
       "         -0.00827384,  0.05188467, -0.13259846, -0.13830307, -0.22628324,\n",
       "          0.14222057,  0.0852554 , -0.08573499,  0.09750654, -0.21631996,\n",
       "          0.18978257,  0.13447599, -0.14888996,  0.1136656 ,  0.1973706 ,\n",
       "          0.09306063, -0.10252227,  0.16944896,  0.20623563, -0.19772647,\n",
       "          0.09350701,  0.19871984,  0.15933077,  0.13309027, -0.13733521,\n",
       "         -0.01989652,  0.01293255, -0.13536319,  0.21334572, -0.1261329 ,\n",
       "         -0.03426142, -0.15557642, -0.2214955 ,  0.04294793, -0.05349791,\n",
       "         -0.09671964, -0.03928621, -0.15031809, -0.16411996,  0.00290333],\n",
       "        [-0.10782209, -0.12832622, -0.04748583, -0.07928482, -0.09680068,\n",
       "          0.15637217, -0.20251074,  0.18073653,  0.16624825,  0.20915292,\n",
       "         -0.20507771,  0.11524652, -0.06872886,  0.04587431,  0.02607958,\n",
       "          0.23535572,  0.12220056, -0.14655347,  0.08555369,  0.0281866 ,\n",
       "          0.00764161, -0.21118797,  0.12009622, -0.10043798, -0.12094204,\n",
       "         -0.14601627, -0.0968191 , -0.14229274, -0.23237032, -0.07108097,\n",
       "          0.13955458, -0.07389329,  0.01792468, -0.08524281,  0.04905234,\n",
       "         -0.15387347, -0.02029012, -0.17440376, -0.17787406, -0.21200104,\n",
       "          0.10042836,  0.02838089, -0.2219483 , -0.05735439,  0.10593967,\n",
       "         -0.18275858,  0.03263979, -0.04652379, -0.12519616, -0.15905893,\n",
       "          0.0850064 , -0.04111432,  0.02319093, -0.02296411, -0.01251321,\n",
       "         -0.21624354, -0.21139914,  0.02908014, -0.22508034, -0.02131525,\n",
       "         -0.1511474 , -0.05226538,  0.04918103,  0.12485786, -0.00768572,\n",
       "         -0.04947774,  0.14080848, -0.22717524, -0.16194932, -0.12413536,\n",
       "         -0.18707101, -0.10507983, -0.03579089, -0.18335746,  0.00165519,\n",
       "          0.11351068, -0.0492146 ,  0.03919671, -0.18046854,  0.22751217,\n",
       "          0.16602032,  0.18672757,  0.00619365,  0.09461267, -0.1404602 ,\n",
       "         -0.05754161,  0.23049335,  0.16074644, -0.12209461,  0.12308009,\n",
       "         -0.0101358 ,  0.17339276,  0.08148502, -0.18130395,  0.01630671,\n",
       "         -0.17528334,  0.20903267,  0.18669789, -0.1494939 ,  0.0266792 ],\n",
       "        [ 0.12557791,  0.05816017, -0.0286911 , -0.09679094, -0.10064201,\n",
       "          0.1596721 ,  0.06798078, -0.20158301,  0.02303331, -0.07536963,\n",
       "         -0.08372554, -0.15455437, -0.07332878, -0.03019071,  0.00429156,\n",
       "          0.09334572,  0.03704478,  0.04992862, -0.0831095 , -0.17174041,\n",
       "          0.05809997, -0.17325792,  0.16268636,  0.16287501, -0.02870621,\n",
       "         -0.06843776,  0.13950334,  0.17873214,  0.23498653,  0.09515388,\n",
       "         -0.01378471,  0.05651985, -0.06625812,  0.02639247, -0.22402136,\n",
       "          0.2291338 , -0.17857279,  0.09984611,  0.23707114, -0.17130391,\n",
       "          0.09907208, -0.10039432,  0.112202  , -0.20267807, -0.17420548,\n",
       "          0.2203712 , -0.11463372,  0.03055967, -0.00877178,  0.10209648,\n",
       "          0.03329687,  0.14219277,  0.11854006,  0.06921868, -0.15227193,\n",
       "         -0.18125819, -0.14923933,  0.10625549,  0.12751178,  0.20771606,\n",
       "         -0.05169789, -0.06888983, -0.02459912,  0.1947455 ,  0.11361007,\n",
       "         -0.07360655,  0.1506014 , -0.0937506 ,  0.10700805,  0.13791613,\n",
       "         -0.19553606, -0.10752061, -0.20917527,  0.20019598, -0.16181783,\n",
       "         -0.23542953,  0.21749078,  0.04599877,  0.02800976, -0.20536363,\n",
       "         -0.21386653,  0.19394584, -0.18818043,  0.0744199 ,  0.10428078,\n",
       "         -0.23303953, -0.1969239 ,  0.16413347, -0.19652689, -0.15948187,\n",
       "          0.17288034, -0.20545626, -0.15331084,  0.11651756,  0.16965042,\n",
       "          0.22753514,  0.09312822, -0.2075089 ,  0.13133733,  0.14775164]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer.variables\n",
    "# x * w + b\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\").\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = metrics_module.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.losses` instead.\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.updates` instead.\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5398901e-05 6.7153485e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softplus: log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))\n",
    "print(customized_softplus([-10., -5., 0., 5., 10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_2 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_3 (Cu (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer.\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        ''''''\n",
    "        # x * w + b. input_shape:[None, a] w:[a, b] output_shape: [None, b]\n",
    "        self.kernel = self.add_weight(name = 'kernel', \n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                     initializer = 'uniform',\n",
    "                                     trainable = True)\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                   shape = (self.units, ),\n",
    "                                   initializer = 'zeros',\n",
    "                                   trainable = True)\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        ''''''\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu',\n",
    "                      input_shape=x_train.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus,\n",
    "    # keras.layers.Dense(1, activation='softplus')\n",
    "    # keras.layers.Dense(1), keras.layers.Activation('softplus')\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2000 - val_loss: 0.6810\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5951 - val_loss: 0.5907\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5210 - val_loss: 0.5258\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4767 - val_loss: 0.4928\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 0.4774\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4691\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4304 - val_loss: 0.4480\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.4365\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4140 - val_loss: 0.4442\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4166\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3993 - val_loss: 0.4235\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3927 - val_loss: 0.4061\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.4020\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.3976\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3811 - val_loss: 0.4049\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.3973\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3933\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3912\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.3886\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.3840\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.3869\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.3907\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                   validation_data=(x_valid_scaled, y_valid),\n",
    "                   epochs=100,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 713us/step - loss: 0.3904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39035630226135254"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
